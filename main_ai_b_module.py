import os
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional
import redis
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import PydanticOutputParser


# --- 1. Pydantic ëª¨ë¸ (API ì…/ì¶œë ¥ ëª…ì„¸) ---

#   BE B/FEê°€ ìš”ì²­í•œëŒ€ë¡œ 'price' í•„ë“œ ì¶”ê°€
class RecommendedMenu(BaseModel):
    """FEì˜ Result í™”ë©´ì— ë°”ì¸ë”©ë  ìµœì¢… ë©”ë‰´ 1ê°œ ì •ë³´"""
    restaurant_name: str = Field(description="ì‹ë‹¹ ì´ë¦„")
    menu_name: str = Field(description="ë©”ë‰´ ì´ë¦„")
    price: int = Field(description="ë©”ë‰´ì˜ ì‹¤ì œ ê°€ê²©")
    justification: str = Field(description="ì´ ë©”ë‰´ë¥¼ ì¶”ì²œí•˜ëŠ” ì´ìœ  (LLMì´ ì‘ì„±)")
    new_score: float = Field(description="LLMì´ ì¬í‰ê°€í•œ ìµœì¢… ì ìˆ˜ (0.0 ~ 1.0)")
    reason_hashtags: List[str] = Field(
        description="ì¶”ì²œ ì´ìœ ë¥¼ ìš”ì•½í•˜ëŠ” 3-5ê°œì˜ í•´ì‹œíƒœê·¸ (ì˜ˆ: ['#ì†í¸í•œ', '#ë“ ë“ í•œ', '#ê°€ì„±ë¹„'])"
    )

#   UI/BE B ìš”ì²­ëŒ€ë¡œ Listê°€ ì•„ë‹Œ 'ë¼ë‹ˆ ìŠ¬ë¡¯' í˜•íƒœë¡œ ë³€ê²½
class FinalRecommendation(BaseModel):
    """ìµœì¢… API ì‘ë‹µ í˜•ì‹ (FEì˜ Result í™”ë©´ê³¼ ì¼ì¹˜)"""
    morning: Optional[RecommendedMenu] = Field(
        default=None, description="ì¶”ì²œ ì•„ì¹¨ ë©”ë‰´ (í•´ë‹¹ ì—†ìœ¼ë©´ null)"
    )
    lunch: Optional[RecommendedMenu] = Field(
        default=None, description="ì¶”ì²œ ì ì‹¬ ë©”ë‰´ (í•´ë‹¹ ì—†ìœ¼ë©´ null)"
    )
    dinner: Optional[RecommendedMenu] = Field(
        default=None, description="ì¶”ì²œ ì €ë… ë©”ë‰´ (í•´ë‹¹ ì—†ìœ¼ë©´ null)"
    )


#   AI A / BE Aê°€ ì „ë‹¬í•  'ê°€ê²©' ì •ë³´ ì¶”ê°€
class MenuCandidate(BaseModel):
    """ë©”ì¸ BE(or AI A)ì—ì„œ ì „ë‹¬ë°›ì„ 1ì°¨ í•„í„°ë§ í›„ë³´"""
    restaurant_name: str
    menu_name: str
    price: int  # LLMì´ ê°€ê²©ì„ ê³ ë ¤í•  ìˆ˜ ìˆë„ë¡ ë©”ë‰´ì˜ ì‹¤ì œ ê°€ê²©
    base_score: float  # AI Aê°€ ë§¤ê¸´ ê¸°ë³¸ ì ìˆ˜
    tags: List[str]


#   BE Aê°€ ìš”ì²­í•œ ê°€ê²© ë²”ìœ„ ê°ì²´
class PriceRange(BaseModel):
    min: Optional[int] = None
    max: Optional[int] = None


#   BE Aê°€ ìš”ì²­í•œ ëª¨ë“  ì •ë³´ë¥¼ ë°›ëŠ” ë©”ì¸ ê°ì²´
class RecommendationRequest(BaseModel):
    """AI B ëª¨ë“ˆì´ ë°›ì„ ìš”ì²­ Body ì „ì²´"""
    candidates: List[MenuCandidate]
    user_prompt: str

    price: Optional[PriceRange] = Field(
        default=None, description="ì‚¬ìš©ì ê°€ê²© ì œí•œ (ì˜ˆ: {'max': 10000})"
    )
    target_meals: List[str] = Field(
        description="ì¶”ì²œë°›ê³ ì í•˜ëŠ” ë¼ë‹ˆ ëª©ë¡ (ì˜ˆ: ['lunch', 'dinner'])"
    )

    conversation_history: Optional[List[str]] = []  # (í™•ì¥ìš©) ëŒ€í™” ì´ë ¥


# --- 2. LangChain ë° LLM ì„¤ì • ---

# OpenAI API í‚¤ ì„¤ì • (í™˜ê²½ ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì˜¤ëŠ” ê²ƒì„ ê¶Œì¥)
# os.environ["OPENAI_API_KEY"] = "sk-..."

# 1. Output Parser (ì¶œë ¥ íŒŒì„œ): LLMì˜ ì‘ë‹µì„ 'FinalRecommendation' JSONìœ¼ë¡œ ê°•ì œ
parser = PydanticOutputParser(pydantic_object=FinalRecommendation)

# 2. LLM ëª¨ë¸: GPT-4 mini ì‚¬ìš©, temperature=0.6 (ëœë¤í•œ ë‹µë³€)
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.6)

# ğŸ’¡ (ìˆ˜ì • 6: í”„ë¡¬í”„íŠ¸) ê°€ê²©, ë¼ë‹ˆ ì •ë³´ë¥¼ ëª¨ë‘ í¬í•¨í•˜ë„ë¡ LLM ì§€ì‹œì„œ ìˆ˜ì •
prompt_template = """
ë‹¹ì‹ ì€ ê³ ë ¤ëŒ€í•™êµ ê·¼ì²˜ ë§›ì§‘ ë©”ë‰´ ì¶”ì²œ AI 'MenuMate'ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì„¸ë¶€ ìš”ì²­ê³¼ AI Aê°€ 1ì°¨ í•„í„°ë§í•œ ë©”ë‰´ í›„ë³´ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤.

[í›„ë³´ ë¦¬ìŠ¤íŠ¸] (ì´ë¦„, ê°€ê²©, AI A ì ìˆ˜, íƒœê·¸ ìˆœ)
{candidates_str}

[ì‚¬ìš©ì ì„¸ë¶€ ìš”ì²­] (ì˜ˆ: ì†í¸í•œ, ë“ ë“ í•œ)
"{user_prompt}"

[ì‚¬ìš©ì ê°€ê²© ì œí•œ]
{price_str}

[ì‚¬ìš©ìê°€ ì¶”ì²œë°›ê¸¸ ì›í•˜ëŠ” ë¼ë‹ˆ]
{target_meals_str}

[ì§€ì‹œ]
1. [í›„ë³´ ë¦¬ìŠ¤íŠ¸] ì¤‘ì—ì„œ [ì‚¬ìš©ì ì„¸ë¶€ ìš”ì²­]ê³¼ [ì‚¬ìš©ì ê°€ê²© ì œí•œ]ì„ ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ë©”ë‰´ë¥¼ ê³ ë¥´ì„¸ìš”.
2. [ì‚¬ìš©ìê°€ ì¶”ì²œë°›ê¸¸ ì›í•˜ëŠ” ë¼ë‹ˆ] ëª©ë¡({target_meals_str})ì— ìˆëŠ” ìŠ¬ë¡¯ì—ë§Œ ì¶”ì²œ ë©”ë‰´ë¥¼ ì±„ì›Œì£¼ì„¸ìš”.
3. ê° í›„ë³´ì˜ 'price' ì •ë³´ë¥¼ [ì¶œë ¥ JSON]ì˜ 'price' í•„ë“œì— ì •í™•íˆ ê¸°ì…í•´ì£¼ì„¸ìš”.
5. ì¶”ì²œ ë©”ë‰´ë¥¼ ì„ ì •í•œ í›„, [ì‚¬ìš©ì ì„¸ë¶€ ìš”ì²­]ì„ ë°”íƒ•ìœ¼ë¡œ ê·¸ ì´ìœ ë¥¼ ìš”ì•½í•˜ëŠ” 1~3ê°œì˜ 'reason_hashtags'ë¥¼ ë°˜ë“œì‹œ ìƒì„±í•´ì£¼ì„¸ìš”. (ì˜ˆ: '#ì†í¸í•œ', '#ë“ ë“ í•œ', '#ê°€ì„±ë¹„')
4. ëª©ë¡ì— ì—†ëŠ” ë‹¤ë¥¸ ë¼ë‹ˆ ìŠ¬ë¡¯ì€ ë°˜ë“œì‹œ nullë¡œ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤.
5. ë§Œì•½ ì‚¬ìš©ìê°€ 'ì ì‹¬'ë§Œ ì›í–ˆë‹¤ë©´ 'morning'ê³¼ 'dinner'ëŠ” ë°˜ë“œì‹œ nullì´ì–´ì•¼ í•©ë‹ˆë‹¤.

ë°˜ë“œì‹œ ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì•¼ í•©ë‹ˆë‹¤ (ë‹¤ë¥¸ ë§ì€ ì ˆëŒ€ í•˜ì§€ ë§ˆì„¸ìš”):
{format_instructions}
"""

# 3. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì™„ì„±
prompt = ChatPromptTemplate.from_template(
    template=prompt_template,
    partial_variables={"format_instructions": parser.get_format_instructions()}
)

# 4. LangChain 'ì²´ì¸' êµ¬ì„± (ì…ë ¥ -> í”„ë¡¬í”„íŠ¸ -> LLM -> JSON íŒŒì„œ)
chain = prompt | llm | parser

# --- 3. FastAPI ì•± ë° ìºì‹± ì„¤ì • ---

app = FastAPI(
    title="MenuMate - AI B Module",
    description="LLMì„ ì´ìš©í•œ ë©”ë‰´ ìµœì¢… ì¶”ì²œ ë° í›„ì²˜ë¦¬ API"
)

# Redis í´ë¼ì´ì–¸íŠ¸ (ì‹¤ì œ ìš´ì˜ ì‹œ host, port, password ë“± ì„¤ì • í•„ìš”)
try:
    cache = redis.Redis(host='localhost', port=6379, db=0, decode_responses=True)
    cache.ping()
    print("Redis ìºì‹œ ì„œë²„ì— ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")
except redis.ConnectionError as e:
    print(f"ê²½ê³ : Redis ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìºì‹±ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤. (ì˜¤ë¥˜: {e})")
    cache = None  # ìºì‹œ ì—°ê²° ì‹¤íŒ¨ ì‹œ Noneìœ¼ë¡œ ì„¤ì •


@app.post("/recommend/refine", response_model=FinalRecommendation)
async def get_refined_recommendations(request: RecommendationRequest):
    """
    AI Aì˜ í›„ë³´ ë¦¬ìŠ¤íŠ¸ì™€ ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ LLMìœ¼ë¡œ ìµœì¢… ë©”ë‰´ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
    """

    # --- 1. ìºì‹± (Caching) ---
    cache_key = None
    if cache:
        try:
            # ìš”ì²­ ê°ì²´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê³ ìœ í•œ ìºì‹œ í‚¤ ìƒì„± (ì•ˆì •ì ì¸ í•´ì‹œ í•„ìš”)
            cache_key = f"recommend:{hash(request.model_dump_json())}"
            cached_result = cache.get(cache_key)
            if cached_result:
                print("ìºì‹œëœ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
                return FinalRecommendation.model_validate_json(cached_result)
        except Exception as e:
            print(f"ìºì‹œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ìºì‹œ ì˜¤ë¥˜ ì‹œ, ê·¸ëƒ¥ LLM í˜¸ì¶œ ì§„í–‰

    # --- 2. LLM ì…ë ¥ê°’ ê°€ê³µ ---

    #   í›„ë³´ ë¦¬ìŠ¤íŠ¸ ë¬¸ìì—´ì— 'ê°€ê²©' í¬í•¨
    candidates_str = "\n".join(
        [f"- {c.restaurant_name} '{c.menu_name}' (ê°€ê²©: {c.price}ì›, ì ìˆ˜: {c.base_score}, íƒœê·¸: {c.tags})"
         for c in request.candidates]
    )

    #   ê°€ê²© ì œí•œ ê°ì²´ë¥¼ LLMì´ ì•Œì•„ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    price_str = "ì œí•œ ì—†ìŒ"
    if request.price:
        parts = []
        if request.price.min is not None:
            parts.append(f"{request.price.min}ì› ì´ìƒ")
        if request.price.max is not None:
            parts.append(f"{request.price.max}ì› ì´í•˜")
        if parts:
            price_str = " ".join(parts)

    #   ëª©í‘œ ë¼ë‹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
    target_meals_str = ", ".join(request.target_meals)  # ì˜ˆ: "lunch, dinner"

    # --- 3. LLM ì²´ì¸ í˜¸ì¶œ (AI Bì˜ í•µì‹¬ ì‘ì—…) ---
    try:
        print(f"LLM í˜¸ì¶œ ì‹œì‘: (User: {request.user_prompt}, Price: {price_str}, Meals: {target_meals_str})")
        result = await chain.ainvoke({
            "candidates_str": candidates_str,
            "user_prompt": request.user_prompt,
            "price_str": price_str,
            "target_meals_str": target_meals_str,
            "history": "\n".join(request.conversation_history)
        })

        # --- 4. ìºì‹œì— ê²°ê³¼ ì €ì¥ ---
        if cache and cache_key:
            try:
                # LLM API ë¹„ìš© ì ˆì•½ì„ ìœ„í•´ 1ì‹œê°„(3600ì´ˆ) ë™ì•ˆ ìºì‹œ
                cache.set(cache_key, result.model_dump_json(), ex=3600)
                print("ìƒˆ ê²°ê³¼ë¥¼ ìƒì„±í•˜ê³  ìºì‹œì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                print(f"ìºì‹œ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

        # --- 5. FEë¡œ ìµœì¢… JSON ë°˜í™˜ ---
        return result

    except Exception as e:
        # --- 6. ì˜¤ë¥˜ ì²˜ë¦¬ ---
        print(f"LLM íŒŒì‹± ë˜ëŠ” API ì˜¤ë¥˜: {e}")
        # (ì‹¤ì œ ìš´ì˜ ì‹œ) ì—¬ê¸°ì— ì˜¤ë¥˜ ë¡œê¹…(Logging) ë¡œì§ ì¶”ê°€
        raise HTTPException(status_code=500, detail=f"ë©”ë‰´ ì¶”ì²œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. (AI B ëª¨ë“ˆ ì˜¤ë¥˜: {e})")


# --- 4. (ì„ íƒ) ì„œë²„ ì‹¤í–‰ ì½”ë“œ ---
if __name__ == "__main__":
    import uvicorn

    # PyCharmì—ì„œ 'ì‹¤í–‰' ë²„íŠ¼ì„ ëˆ„ë¥´ëŠ” ëŒ€ì‹ , í„°ë¯¸ë„ì—ì„œ
    # uvicorn main_ai_b_module:app --reload --port 8000
    # ëª…ë ¹ìœ¼ë¡œ ì‹¤í–‰í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
    uvicorn.run(app, host="127.0.0.1", port=8000)